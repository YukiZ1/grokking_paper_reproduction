{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f746e2",
   "metadata": {},
   "source": [
    "# source: \n",
    "https://youtu.be/D8GOeCFFby4?si=-OmGfudmRexcHa-d\n",
    "\n",
    "OpenAI team’s grokking paper: https://arxiv.org/pdf/2201.02177. I wasn’t able to reach to team for comment on the origin story, but it is told here:    • Girl Geek X OpenAI Lightning Talks  \n",
    "Nanda et al. https://arxiv.org/pdf/2301.05217v1\n",
    "More on Grokking: https://www.quantamagazine.org/how-do...\n",
    "Code based on excellent these notebooks from Neel Nanda and collaborators:\n",
    "https://colab.research.google.com/dri...\n",
    "\n",
    "https://colab.research.google.com/git...\n",
    "Andrej Karapathy on “Summoning Ghosts”: https://karpathy.bearblog.dev/animals...\n",
    "\n",
    "Code: https://github.com/stephencwelch/mani...\n",
    "\n",
    "# Technical Notes\n",
    "It’s very natural for the attention layer to take the sum of it’s inputs (e.g. cos(kx)+cos(ky)), however we also find strong product terms. There’s a couple ways the network can compute products like cos(kx)cos(ky). One option is to approximate the product using ReLU activation functions (see Nanda’s notebooks for more). It’s also feasible for the attention block to do this, I found more evidence of this is my own exploration than of the product happening in the MLP’s ReLU units as suggested by Nanda et al.\n",
    "In the first 2D fourier decomposition, we’re leaving out one component, specifically a “negative frequency component” → 0.26 * np.cos(2*np.pi*((4*i)/113)) * np.cos(2*np.pi*((109*j)/113)). We left this out to avoid digging into a discussion of negative/aliased frequencies, and having this 4th component doesn’t add to our intuition about what the network is doing here.\n",
    "at 28:00 we’re not showing removing the 8pi/113 frequency from the model’s final output surface.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
